{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8707b1d7",
   "metadata": {},
   "source": [
    "### Filtering DBPedia to get a reference dataset\n",
    "\n",
    "#### Steps \n",
    "- *Download the raw DBpedia object mapping dataset from [here](https://databus.dbpedia.org/dbpedia/mappings/mappingbased-objects/2022.12.01/mappingbased-objects_lang=en.ttl.bz2)* \n",
    "- Extract unique subjects and objects from the provided train and test datasets\n",
    "- Filter only the triples, from the reference set, which contain entities present in the train and test sets.\n",
    "- Save.\n",
    "\n",
    "#### Other\n",
    "- A reference dataset maps the given train & test datasets to a vector space\n",
    "- Triplets are filtered based on provided entities (subjects and objects)         \n",
    "- Execution time: 30-60 sec\n",
    "- The reference dataset was download using the following script :  ```data/extract_dbpedia_dataset.sh```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bced57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, RDF\n",
    "from pprint import pprint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ab052bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"data/KG-2022-train.nt.txt\"\n",
    "TEST_FILE = \"data/KG-2022-test.nt.txt\"\n",
    "\n",
    "def get_unique_entities(filename):\n",
    "    g = Graph()\n",
    "    g.parse(filename, format='nt')\n",
    "\n",
    "    unique_p = set()\n",
    "    unique_s = set()\n",
    "    unique_o = set()\n",
    "    unique_sp = set()\n",
    "    unique_po = set()\n",
    "\n",
    "    for  stmt in g.subjects(RDF.type, RDF.Statement):\n",
    "\n",
    "        p = g.value(stmt, RDF.predicate)\n",
    "        s = g.value(stmt, RDF.subject)\n",
    "        o = g.value(stmt, RDF.object)\n",
    "\n",
    "        unique_s.add(str(s))        \n",
    "        unique_p.add(str(p))        \n",
    "        unique_o.add(str(o))        \n",
    "        unique_sp.add((str(s), str(p)))        \n",
    "        unique_po.add((str(p), str(o)))        \n",
    "\n",
    "    return unique_s, unique_p, unique_o, unique_sp,  unique_po\n",
    "\n",
    "train_s, train_p, train_o, train_sp, train_po= get_unique_entities(TRAIN_FILE)\n",
    "test_s, test_p, test_o, test_sp, test_po= get_unique_entities(TEST_FILE)\n",
    "\n",
    "u_s = train_s.union(test_s)\n",
    "u_p = train_p.union(test_p)\n",
    "u_o = train_o.union(test_o)\n",
    "u_sp = train_sp.union(test_sp)\n",
    "u_po = train_po.union(test_po)\n",
    "\n",
    "# Subjects: 1174, Predicates: 711, Objects: 1284\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ebf973a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subjects: 1174, \n",
      "Predicates: 9, \n",
      "Objcts: 1020, \n",
      "Subject Predicate: 1440, \n",
      "Predicate Object: 1319\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Subjects: {len(u_s)}, \n",
    "Predicates: {len(u_p)}, \n",
    "Objcts: {len(u_o)}, \n",
    "Subject Predicate: {len(u_sp)}, \n",
    "Predicate Object: {len(u_po)}\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd5a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import unquote\n",
    "\n",
    "def normalize(uri: str) -> str:\n",
    "    \"\"\"Normalize DBpedia-style URIs for consistent matching.\"\"\"\n",
    "    return unquote(uri.strip(\"<>\"))\n",
    "\n",
    "\n",
    "def filter_reference_kg(reference_file, output_file, u_s, u_p, u_o, u_sp, u_po):\n",
    "    count_kept = 0\n",
    "    total = 0\n",
    "\n",
    "    with open(reference_file, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                s, p, o, _ = line.split(\" \", 3)\n",
    "                # s = normalize(s)\n",
    "                # p = normalize(p)\n",
    "                # o = normalize(o)\n",
    "            except ValueError:\n",
    "                continue  # malformed line\n",
    "\n",
    "            if (\n",
    "                (s in u_s) or\n",
    "                # (p in u_p) or\n",
    "                (o in u_o)\n",
    "                # ((s, p) in u_sp) or\n",
    "                # ((p, 0) in u_po)\n",
    "            ):\n",
    "                fout.write(f\"<{s}> <{p}> <{o}> .\\n\")\n",
    "                count_kept += 1\n",
    "\n",
    "            total += 1\n",
    "\n",
    "    print(\n",
    "        f\"Filtering complete. Kept {count_kept:,} triples \"\n",
    "        f\"({(100 * count_kept / total):.2f}%).\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering complete. Kept 1,139,532 triples (5.00%).\n"
     ]
    }
   ],
   "source": [
    "RAW_FILE = \"data/mappingbased-objects_lang=en.nt\"\n",
    "REFERENCE_FILE = \"data/dbpedia-reference-kg.nt\"\n",
    "if os.path.exists(REFERENCE_FILE):\n",
    "    print(\"Filtered dataset exists !\")\n",
    "else:\n",
    "    filter_reference_kg(RAW_FILE, REFERENCE_FILE, u_s, u_p, u_o, u_sp, u_po)\n",
    "\n",
    "# (data/dbpedia-reference-kg-1.nt) Filtering complete. Kept 1,139,536 triples (5.00%).\n",
    "# Filtering complete. Kept 1,139,532 triples (5.00%)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
